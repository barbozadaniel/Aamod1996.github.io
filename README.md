# Portfolio

I am actively improving my knowledge through online resources. I am currently working on Multi-Agent Reinforcement Learning and Neural Network Optimization Using Genetic Algorithms. Here is a list of some projects which I completed. 

# [Deep Deterministic Policy Gradients](https://github.com/Aamod1996/Deep-Deterministic-Policy-Gradients)

This algorithm is an extension of actor-critic methods to continuous action spaces. These models are very powerful as they combine the best from value based methods and policy based methods into a single algorithm.

# [Deep Q Learning](https://github.com/Aamod1996/Deep-Q-Learning)

Deep Q Learning algorithms is one of the first and also a very powerful algorithm which combines the best from both worlds - Deep Learning and Reinforcement Learning. In this repository I have implemented the algorithm using PyTorch and tested it on an environment from Unity ML Agents. If you want to train your own agent you can follow the instructions in the repository. 

# [Temporal Difference Methods](https://github.com/Aamod1996/Temporal-Difference-Methods)

Temporal Difference methods are a model-free approach of learning from an environment. No prior knowledge of environment is required for these methods. This repository contains the implementation of 3 TD methods - SARSA,  Q-Learning and Expected SARSA. These methods are tested on 'CliffWalking-v0' environment from OpenAI gym environments.

# [Emojifier](https://github.com/Aamod1996/Emojifier)

The emojifier model makes the texts more expressive. Rather than writing "Congratulations on the promotion! Lets get coffee and talk. Love you!" the emojifier can automatically turn this into "Congratulations on the promotion! üëç Lets get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è". The code within this repository contains a simple Emojifier model trained with word embeddings and LSTM network.

# [Deep Cross Entropy Method](https://github.com/Aamod1996/Deep-Cross-Entropy-Method)

Deep Cross Entropy Methods is a very simple yet a very efficient method. You basically generate an episode using your agent and choose the observations which yielded the best results. These observations are used to train the agent and the process continues till convergence. That's how simple it is!

# [Monte Carlo Methods](https://github.com/Aamod1996/Monte-Carlo-Methods)

Monte Carlo Methods search for optimal solutions with the process of repeated random sampling. In this repository I have implemented 4 types of MC methods.
